{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wind ANN "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Avishai\\AppData\\Local\\Temp\\ipykernel_56408\\299724603.py:6: DtypeWarning: Columns (12,17) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv('C:/Users/Avishai/Desktop/Senior-project/Two_Year_Training_Set.csv')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "\n",
    "data = pd.read_csv('C:/Users/Avishai/Desktop/Senior-project/Two_Year_Training_Set.csv')\n",
    "data = data.fillna(0)\n",
    "data['BeginDate'] = pd.to_datetime(data['BeginDate']).dt.tz_localize(None)\n",
    "data[\"Sum\"] = data[[\"Coal\", \"Hydro\", \"Natural Gas\", \"Nuclear\", \"Oil\", \"Other\", \"Landfill Gas\", \"Refuse\", \"Solar\", \"Wind\", \"Wood\"]].sum(axis=1)\n",
    "data['Previous_Day'] = data['BeginDate'] - pd.Timedelta(days=1)\n",
    "data['Previous_2Day'] = data['BeginDate'] - pd.Timedelta(days=2)\n",
    "data['Previous_Year'] = data['BeginDate'] - pd.DateOffset(years=1)\n",
    "wind_data = data[['BeginDate', 'Wind','Previous_Day','Previous_Year','Previous_2Day']].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bisect import bisect_left\n",
    "\n",
    "def get_previous_day_Wind(row, reference_df):\n",
    "    # Sort reference_df by 'BeginDate' for fast lookups\n",
    "    sorted_dates = reference_df['BeginDate'].values\n",
    "    solar_values = reference_df['Wind'].values\n",
    "    \n",
    "    # Perform binary search to find the index of the closest date\n",
    "    target_date = row['Previous_Day']\n",
    "    pos = bisect_left(sorted_dates, target_date)\n",
    "    \n",
    "    # Find the closest date and return corresponding Solar value\n",
    "    if pos == 0:\n",
    "        return solar_values[0]\n",
    "    if pos == len(sorted_dates):\n",
    "        return solar_values[-1]\n",
    "    \n",
    "    before = sorted_dates[pos - 1]\n",
    "    after = sorted_dates[pos]\n",
    "    \n",
    "    # Return the Solar value corresponding to the closest date\n",
    "    if abs(target_date - before) <= abs(target_date - after):\n",
    "        return solar_values[pos - 1]\n",
    "    else:\n",
    "        return solar_values[pos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bisect import bisect_left\n",
    "\n",
    "def get_two_days_before_Wind(row, reference_df):\n",
    "    # Sort reference_df by 'BeginDate' for fast lookups\n",
    "    sorted_dates = reference_df['BeginDate'].values\n",
    "    solar_values = reference_df['Wind'].values\n",
    "    \n",
    "    # Calculate two days before\n",
    "    target_date = row['BeginDate'] - pd.Timedelta(days=2)\n",
    "    \n",
    "    # Perform binary search to find the index of the closest date\n",
    "    pos = bisect_left(sorted_dates, target_date)\n",
    "    \n",
    "    # Find the closest date and return corresponding Solar value\n",
    "    if pos == 0:\n",
    "        return solar_values[0]\n",
    "    if pos == len(sorted_dates):\n",
    "        return solar_values[-1]\n",
    "    \n",
    "    before = sorted_dates[pos - 1]\n",
    "    after = sorted_dates[pos]\n",
    "    \n",
    "    # Return the Solar value corresponding to the closest date\n",
    "    if abs(target_date - before) <= abs(target_date - after):\n",
    "        return solar_values[pos - 1]\n",
    "    else:\n",
    "        return solar_values[pos]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bisect import bisect_left\n",
    "\n",
    "def get_previous_year_Wind(row, reference_df):\n",
    "    # Sort reference_df by 'BeginDate' for fast lookups\n",
    "    sorted_dates = reference_df['BeginDate'].values\n",
    "    solar_values = reference_df['Wind'].values\n",
    "    \n",
    "    # Perform binary search to find the index of the closest date\n",
    "    target_date = row['Previous_Year']\n",
    "    pos = bisect_left(sorted_dates, target_date)\n",
    "    \n",
    "    # Find the closest date and return corresponding Solar value\n",
    "    if pos == 0:\n",
    "        return solar_values[0]\n",
    "    if pos == len(sorted_dates):\n",
    "        return solar_values[-1]\n",
    "    \n",
    "    before = sorted_dates[pos - 1]\n",
    "    after = sorted_dates[pos]\n",
    "    \n",
    "    # Return the Solar value corresponding to the closest date\n",
    "    if abs(target_date - before) <= abs(target_date - after):\n",
    "        return solar_values[pos - 1]\n",
    "    else:\n",
    "        return solar_values[pos]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Large computation \n",
    "data['Previous_Year_Wind'] = data.apply(get_previous_year_Wind, axis=1, reference_df=wind_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff_date = pd.to_datetime(\"2023-10-01\").tz_localize(None)\n",
    "usable_data = data[data['BeginDate'] > cutoff_date].copy()\n",
    "solar_data2 = usable_data[['BeginDate', 'Wind','Previous_Day','Previous_2Day','Previous_Year']].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "usable_data['Previous_Day_Wind'] = usable_data.apply(get_previous_day_Wind, axis=1, reference_df=solar_data2)\n",
    "usable_data['Previous_2Day_Wind'] = usable_data.apply(get_two_days_before_Wind, axis=1, reference_df=solar_data2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features shape:  (93643, 21)\n",
      "Target shape:  (93643,)\n"
     ]
    }
   ],
   "source": [
    "usable_data['Hour_of_Day'] = usable_data['BeginDate'].dt.hour\n",
    "usable_data['Month'] = usable_data['BeginDate'].dt.month\n",
    "usable_data['Year'] = usable_data['BeginDate'].dt.year\n",
    "features = usable_data[['Month','Year','Previous_Year_Wind','Previous_2Day_Wind','Sum','snowdepth','temp','solarenergy','sealevelpressure', 'humidity','solarenergy','snow', 'precip', 'uvindex', 'cloudcover', 'Previous_Day_Wind','Hour_of_Day','dew','windgust','windspeed','winddir']]\n",
    "\n",
    "# Useless Features , ,\n",
    "target = usable_data['Wind']\n",
    "\n",
    "print(\"Features shape: \", features.shape)\n",
    "print('Target shape: ', target.shape)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)\n",
    "\n",
    "scalar = StandardScaler()\n",
    "X_train = scalar.fit_transform(X_train)\n",
    "X_test = scalar.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Avishai\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 5ms/step - loss: 196.0240 - val_loss: 148.2478\n",
      "Epoch 2/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - loss: 148.4822 - val_loss: 142.8413\n",
      "Epoch 3/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 614us/step - loss: 144.3443 - val_loss: 137.8099\n",
      "Epoch 4/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 675us/step - loss: 141.9117 - val_loss: 134.0540\n",
      "Epoch 5/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 609us/step - loss: 138.4266 - val_loss: 133.6870\n",
      "Epoch 6/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 630us/step - loss: 136.1646 - val_loss: 128.0574\n",
      "Epoch 7/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 621us/step - loss: 134.8837 - val_loss: 125.4865\n",
      "Epoch 8/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 612us/step - loss: 133.1622 - val_loss: 124.2460\n",
      "Epoch 9/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 609us/step - loss: 131.2669 - val_loss: 120.9932\n",
      "Epoch 10/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 617us/step - loss: 128.9363 - val_loss: 118.7857\n",
      "Epoch 11/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 614us/step - loss: 127.9870 - val_loss: 118.4188\n",
      "Epoch 12/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 627us/step - loss: 126.8063 - val_loss: 114.9927\n",
      "Epoch 13/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 661us/step - loss: 125.0665 - val_loss: 114.4956\n",
      "Epoch 14/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 628us/step - loss: 124.8456 - val_loss: 112.7445\n",
      "Epoch 15/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 621us/step - loss: 123.8456 - val_loss: 111.3627\n",
      "Epoch 16/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 609us/step - loss: 123.0366 - val_loss: 109.9372\n",
      "Epoch 17/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 629us/step - loss: 121.6217 - val_loss: 108.3574\n",
      "Epoch 18/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 606us/step - loss: 120.2882 - val_loss: 106.5966\n",
      "Epoch 19/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 638us/step - loss: 119.4083 - val_loss: 107.9791\n",
      "Epoch 20/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 609us/step - loss: 119.6832 - val_loss: 105.1011\n",
      "Epoch 21/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 666us/step - loss: 118.2041 - val_loss: 102.5834\n",
      "Epoch 22/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 656us/step - loss: 116.7952 - val_loss: 104.7856\n",
      "Epoch 23/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 663us/step - loss: 116.3424 - val_loss: 102.1030\n",
      "Epoch 24/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 668us/step - loss: 115.8246 - val_loss: 101.5365\n",
      "Epoch 25/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 617us/step - loss: 114.3706 - val_loss: 103.8994\n",
      "Epoch 26/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 715us/step - loss: 114.7902 - val_loss: 98.9646\n",
      "Epoch 27/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 608us/step - loss: 113.6532 - val_loss: 98.5314\n",
      "Epoch 28/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 615us/step - loss: 113.3087 - val_loss: 97.6726\n",
      "Epoch 29/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 610us/step - loss: 112.9288 - val_loss: 97.3211\n",
      "Epoch 30/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 611us/step - loss: 112.5574 - val_loss: 96.0265\n",
      "Epoch 31/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 624us/step - loss: 112.1162 - val_loss: 96.9081\n",
      "Epoch 32/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 600us/step - loss: 110.1849 - val_loss: 95.5641\n",
      "Epoch 33/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 596us/step - loss: 110.9495 - val_loss: 94.6630\n",
      "Epoch 34/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 609us/step - loss: 108.9926 - val_loss: 93.7747\n",
      "Epoch 35/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 601us/step - loss: 108.6831 - val_loss: 93.9810\n",
      "Epoch 36/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 604us/step - loss: 109.4083 - val_loss: 93.5382\n",
      "Epoch 37/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 621us/step - loss: 109.5559 - val_loss: 92.2577\n",
      "Epoch 38/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 641us/step - loss: 108.0109 - val_loss: 91.2741\n",
      "Epoch 39/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 621us/step - loss: 107.9747 - val_loss: 90.6776\n",
      "Epoch 40/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 611us/step - loss: 106.7447 - val_loss: 89.2185\n",
      "Epoch 41/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 645us/step - loss: 106.7577 - val_loss: 89.8149\n",
      "Epoch 42/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 621us/step - loss: 106.9619 - val_loss: 88.6946\n",
      "Epoch 43/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 611us/step - loss: 106.2279 - val_loss: 90.3317\n",
      "Epoch 44/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 622us/step - loss: 105.4371 - val_loss: 90.0091\n",
      "Epoch 45/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 670us/step - loss: 105.5412 - val_loss: 87.0304\n",
      "Epoch 46/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 619us/step - loss: 105.3404 - val_loss: 86.9582\n",
      "Epoch 47/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 624us/step - loss: 104.3525 - val_loss: 89.6742\n",
      "Epoch 48/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 607us/step - loss: 104.4135 - val_loss: 84.9256\n",
      "Epoch 49/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 601us/step - loss: 103.4293 - val_loss: 86.2745\n",
      "Epoch 50/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 597us/step - loss: 103.8474 - val_loss: 84.9717\n",
      "Epoch 51/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 680us/step - loss: 103.3486 - val_loss: 87.0884\n",
      "Epoch 52/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 603us/step - loss: 103.0411 - val_loss: 90.2468\n",
      "Epoch 53/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 596us/step - loss: 102.3758 - val_loss: 84.8501\n",
      "Epoch 54/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 611us/step - loss: 102.6728 - val_loss: 83.2008\n",
      "Epoch 55/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 600us/step - loss: 101.3500 - val_loss: 81.8430\n",
      "Epoch 56/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 607us/step - loss: 100.9121 - val_loss: 82.8048\n",
      "Epoch 57/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 628us/step - loss: 101.4035 - val_loss: 85.0432\n",
      "Epoch 58/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 602us/step - loss: 100.8059 - val_loss: 88.7637\n",
      "Epoch 59/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 595us/step - loss: 99.9503 - val_loss: 83.6655\n",
      "Epoch 60/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 632us/step - loss: 100.2102 - val_loss: 85.1753\n",
      "Epoch 61/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 648us/step - loss: 99.4486 - val_loss: 83.6974\n",
      "Epoch 62/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 601us/step - loss: 100.6394 - val_loss: 84.0676\n",
      "Epoch 63/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 610us/step - loss: 99.2644 - val_loss: 84.4333\n",
      "Epoch 64/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 597us/step - loss: 98.6229 - val_loss: 79.1589\n",
      "Epoch 65/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 629us/step - loss: 98.8303 - val_loss: 82.0425\n",
      "Epoch 66/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 621us/step - loss: 98.8328 - val_loss: 79.7628\n",
      "Epoch 67/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 611us/step - loss: 98.7389 - val_loss: 79.5895\n",
      "Epoch 68/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 594us/step - loss: 98.6361 - val_loss: 80.5452\n",
      "Epoch 69/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 596us/step - loss: 97.5120 - val_loss: 78.2554\n",
      "Epoch 70/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 646us/step - loss: 97.7178 - val_loss: 85.1513\n",
      "Epoch 71/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 608us/step - loss: 96.3260 - val_loss: 80.4238\n",
      "Epoch 72/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 582us/step - loss: 96.6588 - val_loss: 80.7191\n",
      "Epoch 73/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 574us/step - loss: 96.1226 - val_loss: 75.8647\n",
      "Epoch 74/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 579us/step - loss: 95.8973 - val_loss: 77.8867\n",
      "Epoch 75/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 590us/step - loss: 96.4680 - val_loss: 79.5901\n",
      "Epoch 76/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 611us/step - loss: 95.7960 - val_loss: 77.3824\n",
      "Epoch 77/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 592us/step - loss: 95.3984 - val_loss: 77.1270\n",
      "Epoch 78/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 578us/step - loss: 95.1018 - val_loss: 78.6226\n",
      "Epoch 79/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 591us/step - loss: 95.2822 - val_loss: 78.7218\n",
      "Epoch 80/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 650us/step - loss: 94.4619 - val_loss: 77.7183\n",
      "Epoch 81/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 583us/step - loss: 94.7572 - val_loss: 73.8922\n",
      "Epoch 82/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 592us/step - loss: 94.6750 - val_loss: 80.1410\n",
      "Epoch 83/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 630us/step - loss: 94.6600 - val_loss: 82.2001\n",
      "Epoch 84/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 691us/step - loss: 93.9347 - val_loss: 79.3284\n",
      "Epoch 85/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 602us/step - loss: 93.9450 - val_loss: 75.4960\n",
      "Epoch 86/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 588us/step - loss: 93.7340 - val_loss: 72.8094\n",
      "Epoch 87/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 620us/step - loss: 93.5064 - val_loss: 78.6721\n",
      "Epoch 88/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 630us/step - loss: 93.5797 - val_loss: 79.7489\n",
      "Epoch 89/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 576us/step - loss: 93.3115 - val_loss: 80.6603\n",
      "Epoch 90/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 623us/step - loss: 92.3783 - val_loss: 74.1431\n",
      "Epoch 91/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 582us/step - loss: 91.7492 - val_loss: 76.9337\n",
      "Epoch 92/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 610us/step - loss: 93.0055 - val_loss: 75.0049\n",
      "Epoch 93/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 590us/step - loss: 91.3553 - val_loss: 75.3749\n",
      "Epoch 94/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 648us/step - loss: 91.5738 - val_loss: 76.9681\n",
      "Epoch 95/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 605us/step - loss: 91.7505 - val_loss: 74.2896\n",
      "Epoch 96/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 627us/step - loss: 91.6372 - val_loss: 74.2784\n",
      "Epoch 97/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 592us/step - loss: 90.8973 - val_loss: 80.6800\n",
      "Epoch 98/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 601us/step - loss: 91.0355 - val_loss: 75.7764\n",
      "Epoch 99/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 594us/step - loss: 90.7780 - val_loss: 73.9644\n",
      "Epoch 100/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 578us/step - loss: 90.4561 - val_loss: 72.6315\n",
      "Epoch 101/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 591us/step - loss: 90.4420 - val_loss: 74.1686\n",
      "Epoch 102/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 587us/step - loss: 90.3779 - val_loss: 75.9128\n",
      "Epoch 103/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 591us/step - loss: 89.8459 - val_loss: 71.9792\n",
      "Epoch 104/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 644us/step - loss: 89.9610 - val_loss: 72.1518\n",
      "Epoch 105/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 629us/step - loss: 89.5291 - val_loss: 76.4670\n",
      "Epoch 106/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 599us/step - loss: 89.4379 - val_loss: 74.1147\n",
      "Epoch 107/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 660us/step - loss: 88.7166 - val_loss: 71.3708\n",
      "Epoch 108/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 598us/step - loss: 88.9303 - val_loss: 73.2132\n",
      "Epoch 109/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 605us/step - loss: 88.5637 - val_loss: 72.0391\n",
      "Epoch 110/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 597us/step - loss: 89.2302 - val_loss: 69.7606\n",
      "Epoch 111/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 604us/step - loss: 88.1380 - val_loss: 72.4521\n",
      "Epoch 112/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 595us/step - loss: 88.4060 - val_loss: 74.4817\n",
      "Epoch 113/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 589us/step - loss: 88.9330 - val_loss: 73.9045\n",
      "Epoch 114/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 588us/step - loss: 88.1366 - val_loss: 71.3934\n",
      "Epoch 115/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 589us/step - loss: 87.7537 - val_loss: 75.1170\n",
      "Epoch 116/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 587us/step - loss: 87.4064 - val_loss: 72.3217\n",
      "Epoch 117/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 597us/step - loss: 88.1699 - val_loss: 67.8375\n",
      "Epoch 118/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 592us/step - loss: 88.1984 - val_loss: 71.5393\n",
      "Epoch 119/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 601us/step - loss: 86.5389 - val_loss: 74.9180\n",
      "Epoch 120/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 609us/step - loss: 86.8619 - val_loss: 69.5425\n",
      "Epoch 121/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 608us/step - loss: 86.2348 - val_loss: 73.4862\n",
      "Epoch 122/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 599us/step - loss: 87.2011 - val_loss: 72.5910\n",
      "Epoch 123/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 608us/step - loss: 86.3798 - val_loss: 81.4523\n",
      "Epoch 124/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 599us/step - loss: 86.8629 - val_loss: 70.6155\n",
      "Epoch 125/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 611us/step - loss: 86.1627 - val_loss: 70.8056\n",
      "Epoch 126/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 607us/step - loss: 87.2384 - val_loss: 71.7749\n",
      "Epoch 127/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 607us/step - loss: 86.0106 - val_loss: 73.2602\n",
      "Epoch 128/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 610us/step - loss: 86.0040 - val_loss: 70.4278\n",
      "Epoch 129/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 609us/step - loss: 86.2930 - val_loss: 71.3634\n",
      "Epoch 130/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 606us/step - loss: 85.4155 - val_loss: 67.8672\n",
      "Epoch 131/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 610us/step - loss: 85.1404 - val_loss: 75.1135\n",
      "Epoch 132/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 595us/step - loss: 85.3324 - val_loss: 68.4701\n",
      "Epoch 133/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 609us/step - loss: 84.8831 - val_loss: 69.9221\n",
      "Epoch 134/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 602us/step - loss: 84.3389 - val_loss: 70.7614\n",
      "Epoch 135/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 597us/step - loss: 85.0917 - val_loss: 70.8557\n",
      "Epoch 136/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 610us/step - loss: 85.1632 - val_loss: 68.6464\n",
      "Epoch 137/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 610us/step - loss: 85.4592 - val_loss: 72.1404\n",
      "Epoch 138/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 593us/step - loss: 84.3748 - val_loss: 74.6546\n",
      "Epoch 139/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 574us/step - loss: 84.3266 - val_loss: 70.4858\n",
      "Epoch 140/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 584us/step - loss: 85.2089 - val_loss: 69.8088\n",
      "Epoch 141/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 574us/step - loss: 84.6454 - val_loss: 69.1318\n",
      "Epoch 142/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 596us/step - loss: 84.3543 - val_loss: 69.8804\n",
      "Epoch 143/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 594us/step - loss: 84.4592 - val_loss: 66.1975\n",
      "Epoch 144/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 588us/step - loss: 84.2024 - val_loss: 66.6982\n",
      "Epoch 145/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 602us/step - loss: 83.6234 - val_loss: 70.1583\n",
      "Epoch 146/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 593us/step - loss: 83.9433 - val_loss: 70.2521\n",
      "Epoch 147/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 596us/step - loss: 84.4606 - val_loss: 72.5784\n",
      "Epoch 148/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 583us/step - loss: 83.8049 - val_loss: 67.5148\n",
      "Epoch 149/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 586us/step - loss: 83.8406 - val_loss: 75.1447\n",
      "Epoch 150/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 594us/step - loss: 83.9514 - val_loss: 67.7389\n",
      "Epoch 151/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 588us/step - loss: 83.5300 - val_loss: 68.1452\n",
      "Epoch 152/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 591us/step - loss: 83.4134 - val_loss: 77.3152\n",
      "Epoch 153/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 595us/step - loss: 83.5809 - val_loss: 72.1002\n",
      "Epoch 154/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 598us/step - loss: 83.9971 - val_loss: 77.4076\n",
      "Epoch 155/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 602us/step - loss: 83.5420 - val_loss: 73.1153\n",
      "Epoch 156/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 608us/step - loss: 83.0611 - val_loss: 68.5311\n",
      "Epoch 157/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 607us/step - loss: 83.3114 - val_loss: 69.1760\n",
      "Epoch 158/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 587us/step - loss: 83.4275 - val_loss: 73.7439\n",
      "Epoch 159/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 591us/step - loss: 83.0246 - val_loss: 65.9112\n",
      "Epoch 160/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 595us/step - loss: 82.6018 - val_loss: 71.2563\n",
      "Epoch 161/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 591us/step - loss: 82.9028 - val_loss: 68.4608\n",
      "Epoch 162/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 602us/step - loss: 81.9635 - val_loss: 71.2523\n",
      "Epoch 163/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 598us/step - loss: 82.4082 - val_loss: 68.2662\n",
      "Epoch 164/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 602us/step - loss: 83.0323 - val_loss: 69.2293\n",
      "Epoch 165/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 599us/step - loss: 82.0527 - val_loss: 67.6629\n",
      "Epoch 166/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 590us/step - loss: 82.4175 - val_loss: 68.4373\n",
      "Epoch 167/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 590us/step - loss: 82.0378 - val_loss: 64.7769\n",
      "Epoch 168/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 588us/step - loss: 82.8631 - val_loss: 68.9376\n",
      "Epoch 169/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 593us/step - loss: 82.3937 - val_loss: 72.3506\n",
      "Epoch 170/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 610us/step - loss: 82.7255 - val_loss: 70.0168\n",
      "Epoch 171/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 610us/step - loss: 82.0156 - val_loss: 72.3498\n",
      "Epoch 172/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 598us/step - loss: 81.9782 - val_loss: 72.4526\n",
      "Epoch 173/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 608us/step - loss: 82.2584 - val_loss: 68.4630\n",
      "Epoch 174/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 615us/step - loss: 81.9584 - val_loss: 67.5583\n",
      "Epoch 175/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 599us/step - loss: 82.2797 - val_loss: 66.9728\n",
      "Epoch 176/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 605us/step - loss: 81.9565 - val_loss: 72.3888\n",
      "Epoch 177/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 610us/step - loss: 81.6948 - val_loss: 69.0499\n",
      "Epoch 178/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 621us/step - loss: 81.1383 - val_loss: 66.6791\n",
      "Epoch 179/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 608us/step - loss: 81.3836 - val_loss: 68.9167\n",
      "Epoch 180/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 614us/step - loss: 80.9634 - val_loss: 70.9832\n",
      "Epoch 181/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 599us/step - loss: 81.6704 - val_loss: 72.3464\n",
      "Epoch 182/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 597us/step - loss: 81.3668 - val_loss: 71.6467\n",
      "Epoch 183/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 601us/step - loss: 80.8389 - val_loss: 71.5289\n",
      "Epoch 184/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 597us/step - loss: 81.4458 - val_loss: 78.9986\n",
      "Epoch 185/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 584us/step - loss: 81.0480 - val_loss: 74.0226\n",
      "Epoch 186/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 589us/step - loss: 80.8619 - val_loss: 72.5032\n",
      "Epoch 187/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 594us/step - loss: 81.3658 - val_loss: 68.6914\n",
      "Epoch 188/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 609us/step - loss: 81.3236 - val_loss: 72.7568\n",
      "Epoch 189/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 611us/step - loss: 80.8489 - val_loss: 72.6051\n",
      "Epoch 190/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 620us/step - loss: 80.6812 - val_loss: 70.1205\n",
      "Epoch 191/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 614us/step - loss: 81.3378 - val_loss: 74.8359\n",
      "Epoch 192/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 603us/step - loss: 80.9395 - val_loss: 78.9559\n",
      "Epoch 193/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 610us/step - loss: 80.8396 - val_loss: 77.8857\n",
      "Epoch 194/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 619us/step - loss: 80.8192 - val_loss: 79.0609\n",
      "Epoch 195/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 596us/step - loss: 80.6799 - val_loss: 72.6478\n",
      "Epoch 196/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 609us/step - loss: 80.6753 - val_loss: 76.7498\n",
      "Epoch 197/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 615us/step - loss: 81.0165 - val_loss: 76.4772\n",
      "Epoch 198/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 605us/step - loss: 80.1767 - val_loss: 75.4255\n",
      "Epoch 199/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 597us/step - loss: 80.9427 - val_loss: 76.2137\n",
      "Epoch 200/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 590us/step - loss: 79.9960 - val_loss: 73.2767\n",
      "Epoch 201/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 596us/step - loss: 80.0257 - val_loss: 78.0925\n",
      "Epoch 202/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 589us/step - loss: 80.0387 - val_loss: 69.7496\n",
      "Epoch 203/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 577us/step - loss: 80.4110 - val_loss: 79.8506\n",
      "Epoch 204/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 581us/step - loss: 79.2995 - val_loss: 74.0099\n",
      "Epoch 205/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 575us/step - loss: 79.6968 - val_loss: 72.3459\n",
      "Epoch 206/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 564us/step - loss: 79.7043 - val_loss: 79.2952\n",
      "Epoch 207/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 568us/step - loss: 79.6090 - val_loss: 82.1090\n",
      "Epoch 208/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 593us/step - loss: 79.8239 - val_loss: 75.3335\n",
      "Epoch 209/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 595us/step - loss: 79.5763 - val_loss: 78.1677\n",
      "Epoch 210/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 589us/step - loss: 78.9666 - val_loss: 75.7221\n",
      "Epoch 211/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 602us/step - loss: 78.7934 - val_loss: 81.7771\n",
      "Epoch 212/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 606us/step - loss: 78.9292 - val_loss: 80.3093\n",
      "Epoch 213/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 606us/step - loss: 79.3706 - val_loss: 78.5384\n",
      "Epoch 214/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 608us/step - loss: 79.6690 - val_loss: 79.2801\n",
      "Epoch 215/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 614us/step - loss: 79.0142 - val_loss: 76.8741\n",
      "Epoch 216/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 607us/step - loss: 79.6584 - val_loss: 76.8444\n",
      "Epoch 217/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 608us/step - loss: 79.2926 - val_loss: 84.4902\n",
      "Epoch 218/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 597us/step - loss: 78.4652 - val_loss: 81.8275\n",
      "Epoch 219/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 598us/step - loss: 78.5226 - val_loss: 82.3973\n",
      "Epoch 220/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 602us/step - loss: 77.9143 - val_loss: 85.4612\n",
      "Epoch 221/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 598us/step - loss: 78.7099 - val_loss: 80.6456\n",
      "Epoch 222/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 597us/step - loss: 78.3731 - val_loss: 83.7149\n",
      "Epoch 223/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 603us/step - loss: 78.5092 - val_loss: 80.2580\n",
      "Epoch 224/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 602us/step - loss: 78.1261 - val_loss: 81.2246\n",
      "Epoch 225/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 611us/step - loss: 78.9346 - val_loss: 79.0733\n",
      "Epoch 226/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 605us/step - loss: 78.3841 - val_loss: 75.8664\n",
      "Epoch 227/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 607us/step - loss: 77.6104 - val_loss: 83.1213\n",
      "Epoch 228/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 621us/step - loss: 78.5069 - val_loss: 80.6316\n",
      "Epoch 229/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 619us/step - loss: 77.9071 - val_loss: 73.7141\n",
      "Epoch 230/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 613us/step - loss: 78.3019 - val_loss: 81.0471\n",
      "Epoch 231/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 596us/step - loss: 78.2002 - val_loss: 80.0026\n",
      "Epoch 232/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 604us/step - loss: 77.7060 - val_loss: 80.1974\n",
      "Epoch 233/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 606us/step - loss: 77.6679 - val_loss: 83.1623\n",
      "Epoch 234/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 610us/step - loss: 77.4017 - val_loss: 84.7361\n",
      "Epoch 235/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 616us/step - loss: 77.3588 - val_loss: 82.6877\n",
      "Epoch 236/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 606us/step - loss: 78.1554 - val_loss: 78.4973\n",
      "Epoch 237/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 595us/step - loss: 77.7665 - val_loss: 77.1306\n",
      "Epoch 238/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 606us/step - loss: 76.9501 - val_loss: 79.5959\n",
      "Epoch 239/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 610us/step - loss: 76.8184 - val_loss: 77.4348\n",
      "Epoch 240/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 602us/step - loss: 77.9244 - val_loss: 81.9083\n",
      "Epoch 241/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 605us/step - loss: 78.2763 - val_loss: 79.0555\n",
      "Epoch 242/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 607us/step - loss: 78.0589 - val_loss: 77.3142\n",
      "Epoch 243/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 595us/step - loss: 77.1577 - val_loss: 78.4708\n",
      "Epoch 244/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 608us/step - loss: 77.4086 - val_loss: 74.7478\n",
      "Epoch 245/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 614us/step - loss: 77.5007 - val_loss: 83.9754\n",
      "Epoch 246/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 604us/step - loss: 77.7101 - val_loss: 82.2554\n",
      "Epoch 247/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 603us/step - loss: 77.0308 - val_loss: 79.0243\n",
      "Epoch 248/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 604us/step - loss: 76.9598 - val_loss: 75.4957\n",
      "Epoch 249/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 597us/step - loss: 76.8930 - val_loss: 79.1269\n",
      "Epoch 250/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 597us/step - loss: 77.2603 - val_loss: 80.6831\n",
      "Epoch 251/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 593us/step - loss: 77.1760 - val_loss: 77.5384\n",
      "Epoch 252/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 596us/step - loss: 76.7200 - val_loss: 74.2514\n",
      "Epoch 253/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 602us/step - loss: 77.6159 - val_loss: 78.4529\n",
      "Epoch 254/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 588us/step - loss: 76.7085 - val_loss: 74.0317\n",
      "Epoch 255/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 592us/step - loss: 76.3050 - val_loss: 79.6027\n",
      "Epoch 256/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 590us/step - loss: 76.7494 - val_loss: 75.6364\n",
      "Epoch 257/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 588us/step - loss: 76.5805 - val_loss: 79.2325\n",
      "Epoch 258/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 593us/step - loss: 76.9529 - val_loss: 78.4945\n",
      "Epoch 259/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 580us/step - loss: 76.6426 - val_loss: 81.2160\n",
      "Epoch 260/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 585us/step - loss: 76.9679 - val_loss: 75.9319\n",
      "Epoch 261/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 580us/step - loss: 76.5680 - val_loss: 77.0071\n",
      "Epoch 262/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 585us/step - loss: 76.9744 - val_loss: 81.6475\n",
      "Epoch 263/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 586us/step - loss: 75.9750 - val_loss: 77.5031\n",
      "Epoch 264/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 592us/step - loss: 76.3862 - val_loss: 82.8969\n",
      "Epoch 265/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 590us/step - loss: 76.5839 - val_loss: 83.0631\n",
      "Epoch 266/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 584us/step - loss: 76.7534 - val_loss: 81.7866\n",
      "Epoch 267/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 585us/step - loss: 76.5232 - val_loss: 81.9966\n",
      "Epoch 268/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 585us/step - loss: 76.4171 - val_loss: 77.3078\n",
      "Epoch 269/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 587us/step - loss: 76.4099 - val_loss: 82.2577\n",
      "Epoch 270/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 581us/step - loss: 76.3103 - val_loss: 76.6889\n",
      "Epoch 271/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 583us/step - loss: 76.6609 - val_loss: 79.4010\n",
      "Epoch 272/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 583us/step - loss: 76.4092 - val_loss: 78.1178\n",
      "Epoch 273/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 579us/step - loss: 75.9645 - val_loss: 81.2842\n",
      "Epoch 274/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 576us/step - loss: 76.4397 - val_loss: 83.0164\n",
      "Epoch 275/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 583us/step - loss: 76.2904 - val_loss: 81.3867\n",
      "Epoch 276/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 591us/step - loss: 76.0845 - val_loss: 80.4855\n",
      "Epoch 277/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 601us/step - loss: 75.7414 - val_loss: 80.6349\n",
      "Epoch 278/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 594us/step - loss: 76.0298 - val_loss: 80.9636\n",
      "Epoch 279/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 593us/step - loss: 75.6281 - val_loss: 77.9948\n",
      "Epoch 280/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 591us/step - loss: 75.7622 - val_loss: 79.6155\n",
      "Epoch 281/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 592us/step - loss: 75.9135 - val_loss: 83.5029\n",
      "Epoch 282/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 595us/step - loss: 76.0835 - val_loss: 82.5587\n",
      "Epoch 283/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 602us/step - loss: 75.9445 - val_loss: 80.1032\n",
      "Epoch 284/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 600us/step - loss: 75.4290 - val_loss: 77.1380\n",
      "Epoch 285/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 605us/step - loss: 75.9766 - val_loss: 86.4428\n",
      "Epoch 286/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 600us/step - loss: 76.4131 - val_loss: 80.7338\n",
      "Epoch 287/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 594us/step - loss: 76.1784 - val_loss: 76.3057\n",
      "Epoch 288/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 592us/step - loss: 76.3983 - val_loss: 79.8659\n",
      "Epoch 289/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 604us/step - loss: 76.0190 - val_loss: 79.9051\n",
      "Epoch 290/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 605us/step - loss: 74.9222 - val_loss: 78.7948\n",
      "Epoch 291/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 601us/step - loss: 76.1647 - val_loss: 79.7466\n",
      "Epoch 292/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 595us/step - loss: 75.7621 - val_loss: 83.1847\n",
      "Epoch 293/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 600us/step - loss: 75.1025 - val_loss: 78.1629\n",
      "Epoch 294/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 603us/step - loss: 75.2801 - val_loss: 80.8251\n",
      "Epoch 295/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 598us/step - loss: 75.5587 - val_loss: 81.4596\n",
      "Epoch 296/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 594us/step - loss: 75.9517 - val_loss: 77.8195\n",
      "Epoch 297/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 595us/step - loss: 75.2083 - val_loss: 77.6137\n",
      "Epoch 298/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 594us/step - loss: 75.8727 - val_loss: 82.5544\n",
      "Epoch 299/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 602us/step - loss: 75.5103 - val_loss: 79.8888\n",
      "Epoch 300/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 601us/step - loss: 76.0387 - val_loss: 76.8314\n",
      "Epoch 301/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 599us/step - loss: 75.3206 - val_loss: 84.4210\n",
      "Epoch 302/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 616us/step - loss: 75.5317 - val_loss: 77.2534\n",
      "Epoch 303/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 603us/step - loss: 75.7272 - val_loss: 78.1565\n",
      "Epoch 304/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 603us/step - loss: 74.9296 - val_loss: 81.6816\n",
      "Epoch 305/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 602us/step - loss: 75.1840 - val_loss: 79.4333\n",
      "Epoch 306/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 606us/step - loss: 74.8819 - val_loss: 79.8381\n",
      "Epoch 307/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 596us/step - loss: 75.6212 - val_loss: 79.7855\n",
      "Epoch 308/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 591us/step - loss: 75.4413 - val_loss: 77.7859\n",
      "Epoch 309/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 606us/step - loss: 75.2855 - val_loss: 78.5975\n",
      "Epoch 310/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 605us/step - loss: 75.5904 - val_loss: 76.8610\n",
      "Epoch 311/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 601us/step - loss: 75.0284 - val_loss: 73.8700\n",
      "Epoch 312/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 616us/step - loss: 75.1315 - val_loss: 84.1833\n",
      "Epoch 313/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 612us/step - loss: 75.6876 - val_loss: 81.1781\n",
      "Epoch 314/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 602us/step - loss: 74.9051 - val_loss: 79.5887\n",
      "Epoch 315/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 599us/step - loss: 74.9025 - val_loss: 80.2808\n",
      "Epoch 316/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 611us/step - loss: 75.1810 - val_loss: 75.1914\n",
      "Epoch 317/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 604us/step - loss: 75.2201 - val_loss: 80.5182\n",
      "Epoch 318/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 604us/step - loss: 75.5630 - val_loss: 80.6507\n",
      "Epoch 319/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 604us/step - loss: 75.6685 - val_loss: 76.3091\n",
      "Epoch 320/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 621us/step - loss: 74.6116 - val_loss: 81.6335\n",
      "Epoch 321/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 602us/step - loss: 75.1871 - val_loss: 85.1682\n",
      "Epoch 322/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 604us/step - loss: 75.0691 - val_loss: 81.6231\n",
      "Epoch 323/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 593us/step - loss: 74.7844 - val_loss: 79.4236\n",
      "Epoch 324/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 599us/step - loss: 74.5580 - val_loss: 77.3892\n",
      "Epoch 325/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 598us/step - loss: 74.9571 - val_loss: 79.6684\n",
      "Epoch 326/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 601us/step - loss: 75.2553 - val_loss: 88.2225\n",
      "Epoch 327/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 611us/step - loss: 74.8429 - val_loss: 78.6224\n",
      "Epoch 328/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 598us/step - loss: 74.2566 - val_loss: 77.5108\n",
      "Epoch 329/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 597us/step - loss: 74.8315 - val_loss: 80.5811\n",
      "Epoch 330/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 599us/step - loss: 74.5786 - val_loss: 81.9080\n",
      "Epoch 331/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 582us/step - loss: 74.6753 - val_loss: 78.0027\n",
      "Epoch 332/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 596us/step - loss: 74.2928 - val_loss: 79.2184\n",
      "Epoch 333/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 597us/step - loss: 74.5471 - val_loss: 80.1972\n",
      "Epoch 334/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 593us/step - loss: 74.9163 - val_loss: 75.6364\n",
      "Epoch 335/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 596us/step - loss: 74.8295 - val_loss: 76.4891\n",
      "Epoch 336/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 586us/step - loss: 74.1862 - val_loss: 82.0897\n",
      "Epoch 337/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 597us/step - loss: 74.6123 - val_loss: 84.1971\n",
      "Epoch 338/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 583us/step - loss: 75.1975 - val_loss: 79.2376\n",
      "Epoch 339/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 596us/step - loss: 74.2123 - val_loss: 79.0652\n",
      "Epoch 340/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 587us/step - loss: 75.0767 - val_loss: 82.1481\n",
      "Epoch 341/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 626us/step - loss: 74.6620 - val_loss: 80.7262\n",
      "Epoch 342/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 590us/step - loss: 74.4781 - val_loss: 79.5257\n",
      "Epoch 343/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 595us/step - loss: 75.1471 - val_loss: 80.3252\n",
      "Epoch 344/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 598us/step - loss: 74.5505 - val_loss: 81.3119\n",
      "Epoch 345/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 590us/step - loss: 74.2448 - val_loss: 78.8336\n",
      "Epoch 346/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 625us/step - loss: 74.3176 - val_loss: 80.0754\n",
      "Epoch 347/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 695us/step - loss: 74.3420 - val_loss: 79.2236\n",
      "Epoch 348/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 636us/step - loss: 74.6145 - val_loss: 85.9584\n",
      "Epoch 349/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 624us/step - loss: 74.7487 - val_loss: 80.1384\n",
      "Epoch 350/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 598us/step - loss: 74.7136 - val_loss: 77.3609\n",
      "Epoch 351/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 74.4170 - val_loss: 85.0405\n",
      "Epoch 352/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 752us/step - loss: 74.4665 - val_loss: 77.1052\n",
      "Epoch 353/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 754us/step - loss: 74.2779 - val_loss: 79.8440\n",
      "Epoch 354/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 692us/step - loss: 74.4346 - val_loss: 75.1891\n",
      "Epoch 355/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 712us/step - loss: 74.0570 - val_loss: 77.3134\n",
      "Epoch 356/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 645us/step - loss: 74.4945 - val_loss: 84.3593\n",
      "Epoch 357/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 627us/step - loss: 74.3553 - val_loss: 77.2971\n",
      "Epoch 358/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 628us/step - loss: 74.4614 - val_loss: 85.6629\n",
      "Epoch 359/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 637us/step - loss: 74.4373 - val_loss: 79.4308\n",
      "Epoch 360/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 630us/step - loss: 74.4928 - val_loss: 82.8220\n",
      "Epoch 361/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 649us/step - loss: 74.5813 - val_loss: 86.7317\n",
      "Epoch 362/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 617us/step - loss: 74.1794 - val_loss: 77.1881\n",
      "Epoch 363/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 617us/step - loss: 73.7966 - val_loss: 83.6852\n",
      "Epoch 364/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 636us/step - loss: 74.3407 - val_loss: 82.1815\n",
      "Epoch 365/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 669us/step - loss: 74.8873 - val_loss: 79.0233\n",
      "Epoch 366/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 701us/step - loss: 73.8347 - val_loss: 76.7905\n",
      "Epoch 367/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 623us/step - loss: 74.3202 - val_loss: 77.7958\n",
      "Epoch 368/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 618us/step - loss: 73.7226 - val_loss: 85.3444\n",
      "Epoch 369/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 615us/step - loss: 74.3472 - val_loss: 76.3578\n",
      "Epoch 370/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 615us/step - loss: 74.3551 - val_loss: 82.5586\n",
      "Epoch 371/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 618us/step - loss: 73.9996 - val_loss: 75.0245\n",
      "Epoch 372/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 623us/step - loss: 74.6953 - val_loss: 84.2174\n",
      "Epoch 373/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 626us/step - loss: 74.7004 - val_loss: 86.0969\n",
      "Epoch 374/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 629us/step - loss: 73.6614 - val_loss: 75.2566\n",
      "Epoch 375/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 652us/step - loss: 74.7868 - val_loss: 78.8326\n",
      "Epoch 376/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 635us/step - loss: 73.9290 - val_loss: 79.0949\n",
      "Epoch 377/500\n",
      "\u001b[1m1990/1990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 647us/step - loss: 74.4656 - val_loss: 79.7308\n",
      "Epoch 378/500\n",
      "\u001b[1m1010/1990\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 564us/step - loss: 74.0966"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 17\u001b[0m\n\u001b[0;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mSequential([\n\u001b[0;32m      4\u001b[0m     tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mDense(\u001b[38;5;241m128\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m, input_shape\u001b[38;5;241m=\u001b[39m(X_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m],),\n\u001b[0;32m      5\u001b[0m                           kernel_regularizer\u001b[38;5;241m=\u001b[39mregularizers\u001b[38;5;241m.\u001b[39ml2(\u001b[38;5;241m0.001\u001b[39m)),  \u001b[38;5;66;03m# L2 regularization\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m     tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mDense(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     13\u001b[0m ])\n\u001b[0;32m     15\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean_absolute_error\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 17\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.15\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m test_loss \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(X_test, y_test)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTest Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:320\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator\u001b[38;5;241m.\u001b[39menumerate_epoch():\n\u001b[0;32m    319\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m--> 320\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    321\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n\u001b[0;32m    322\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[1;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[0;32m    880\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[0;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1324\u001b[0m     args,\n\u001b[0;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1326\u001b[0m     executing_eagerly)\n\u001b[0;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\eager\\context.py:1683\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1681\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1682\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1683\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1684\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1685\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1686\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1687\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1688\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1689\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1690\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1691\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1692\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1693\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1697\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1698\u001b[0m   )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'), \n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=700, validation_split=0.15, batch_size=32)\n",
    "\n",
    "test_loss = model.evaluate(X_test, y_test)\n",
    "print(f'Test Loss: {test_loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m586/586\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "Predicted Solar Energy Ouput: [1002.5513305664062, 46.98427963256836, 464.2265319824219, 102.72356414794922, 95.91348266601562, 980.8984985351562, 838.8299560546875, 465.2689514160156, 400.0727844238281, 398.5088806152344, 629.7739868164062, 184.55877685546875, 659.1001586914062, 211.14390563964844, 279.7440490722656, 341.7652893066406, 117.65876007080078, 76.1279296875, 258.7194519042969, 45.20821762084961]\n",
      "Actual Solar Energy Output: [1033.   51.  448.  104.   96.  983.  792.  420.  445.  382.  605.  171.\n",
      "  655.  167.  289.  325.  108.   89.  237.   34.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "predictions = model.predict(X_test)\n",
    "predictions[predictions < 0] = 0\n",
    "flattened_predictions = [0 if (isinstance(pred, np.ndarray) and pred.item() < 0) else (0 if pred < 0 else pred.item() if isinstance(pred, np.ndarray) else pred) for pred in predictions]\n",
    "\n",
    "print(f'Predicted Solar Energy Ouput: {flattened_predictions[:20]}')\n",
    "print(f'Actual Solar Energy Output: {y_test[:20].values}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 24.259763524411255\n",
      "Mean Squared Error (MSE): 1024.8424252624964\n",
      "Root Mean Squared Error (RMSE): 32.013160188623935\n",
      "Percent Error (PERR): 0.06501779285319724\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# Assuming predictions and y_test are numpy arrays or pandas series\n",
    "# predictions = model.predict(X_test)\n",
    "\n",
    "# Calculate Mean Absolute Error (MAE)\n",
    "mae = mean_absolute_error(y_test, predictions)\n",
    "\n",
    "# Calculate Mean Squared Error (MSE)\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "\n",
    "# Calculate Root Mean Squared Error (RMSE)\n",
    "rmse = np.sqrt(mse)\n",
    "average_y_test = np.mean(y_test)\n",
    "percent_error = mae / average_y_test\n",
    "\n",
    "# Display results\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
    "print(f\"Percent Error (PERR): {percent_error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model.save('WindModel.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
